{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "import ast\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the list of repository urls to crawl\n",
    "repository_list = ['https://github.com/matplotlib/matplotlib.git',\n",
    "                    'https://github.com/scikit-learn/scikit-learn.git',\n",
    "                    'https://github.com/numpy/numpy.git',\n",
    "                    'https://github.com/scipy/scipy.git',\n",
    "                    'https://github.com/pallets/flask.git',\n",
    "                    'https://github.com/psf/requests.git',\n",
    "                    'https://github.com/scrapy/scrapy']\n",
    "\n",
    "# specify the path of the output text file\n",
    "output_file = \"python_files.txt\"\n",
    "\n",
    "# loop through all repository urls\n",
    "for url in repository_list:\n",
    "    # clone the repository to a temporary directory\n",
    "    repo_dir = os.path.join(os.getcwd(), \"temp\")\n",
    "    Repo.clone_from(url, repo_dir)\n",
    "\n",
    "    # loop through all Python files in the repository\n",
    "    for root, dirs, files in os.walk(repo_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                # get the contents of the Python file\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "                    file_content = f.read()\n",
    "                # write the contents of the Python file to the output file\n",
    "                with open(output_file, \"a\", encoding='utf-8') as f:\n",
    "                    f.write(f\"Repository: {url}\\nFile: {file_path}\\n\\n{file_content}\\n\\n{'-'*50}\\n\\n\")\n",
    "\n",
    "    # delete the temporary directory\n",
    "    os.system(f\"rm -rf {repo_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 1439996\n"
     ]
    }
   ],
   "source": [
    "# Count the number of lines of code in this file\n",
    "num_lines = 0\n",
    "\n",
    "with open('python_files.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    num_lines = len(lines)\n",
    "\n",
    "print(f\"Lines: {num_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 5978486\n"
     ]
    }
   ],
   "source": [
    "# Creates a function that counts the number of words in output file\n",
    "def tokenize(path):\n",
    "\n",
    "    token_list = []\n",
    "\n",
    "    # count the number of tokens in the file\n",
    "    with open(path, \"r\", encoding='utf-8') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    tokens = re.findall(r'\\b\\w+\\b', file_content)\n",
    "    token_list.append(tokens)\n",
    "    num_tokens = len(tokens)\n",
    "    print(f\"Number of tokens: {num_tokens}\")\n",
    "\n",
    "    return token_list\n",
    "\n",
    "tokenized_list = tokenize(\"python_files.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 29892430)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train your Gensim Word2Vec model with the tokenized lines of code\n",
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2\n",
    ")\n",
    "\n",
    "model.build_vocab(tokenized_list)\n",
    "\n",
    "model.train(tokenized_list, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 0.9962059855461121),\n",
       " ('not', 0.9961488842964172),\n",
       " ('f', 0.996028482913971),\n",
       " ('in', 0.9957643151283264),\n",
       " ('the', 0.9957113862037659),\n",
       " ('name', 0.9956578612327576),\n",
       " ('return', 0.9954971075057983),\n",
       " ('if', 0.9953508377075195),\n",
       " ('sphinx', 0.9949526190757751),\n",
       " ('this', 0.9948042035102844)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explored the trained model by examining the closest_words to “for”\n",
    "model.wv.most_similar('for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('in', 0.9966590404510498),\n",
       " ('to', 0.9966531991958618),\n",
       " ('f', 0.9965929388999939),\n",
       " ('return', 0.9965319633483887),\n",
       " ('app', 0.9964976906776428),\n",
       " ('not', 0.9964142441749573),\n",
       " ('name', 0.9961705207824707),\n",
       " ('the', 0.9958152174949646),\n",
       " ('path', 0.9957109689712524),\n",
       " ('as', 0.9956626892089844)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explored the trained model by examining the closest_words to “if”\n",
    "model.wv.most_similar('if')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32239005"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examining the popular identifier names like \"math\" and \"numpy\" and the similarity between them\n",
    "model.wv.similarity('math','numpy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np: 4721\n",
      "plt: 3864\n",
      "ax: 2673\n",
      "fig: 1659\n",
      "X: 1301\n",
      "y: 1206\n",
      "x: 1059\n",
      "axs: 766\n",
      "def_gen: 709\n",
      "ax1: 539\n",
      "random_st: 504\n",
      "ax2: 462\n",
      "print: 441\n",
      "t: 375\n",
      "A: 316\n",
      "Y: 283\n",
      "rng: 280\n",
      "D_arr_0p5: 271\n",
      "__all__: 268\n",
      "X_train: 249\n",
      "n_samples: 240\n",
      "Z: 238\n",
      "len: 226\n",
      "X_test: 219\n",
      "D_arr_like_0p5: 217\n",
      "iris: 215\n",
      "data: 206\n",
      "clf: 190\n",
      "i8: 188\n",
      "s: 184\n",
      "re: 180\n",
      "time: 179\n",
      "y_train: 175\n",
      "i: 159\n",
      "b_: 157\n",
      "int: 156\n",
      "__name__: 152\n",
      "y_test: 142\n",
      "i4: 138\n",
      "z: 135\n",
      "_: 128\n",
      "xx: 128\n",
      "N: 128\n",
      "ax3: 127\n",
      "float: 127\n",
      "os: 126\n",
      "b: 126\n",
      "a: 120\n",
      "ax0: 118\n",
      "c: 116\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "# specify the path of the Python file to be analyzed\n",
    "file_path = \"python_files.txt\"\n",
    "\n",
    "# read the file and get the abstract syntax tree\n",
    "with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "    code = f.read()\n",
    "\n",
    "# get a list of all identifiers in the ast tree\n",
    "identifiers = []\n",
    "for line in code.splitlines():\n",
    "    try:\n",
    "        # get the abstract syntax tree of the file\n",
    "        ast_tree = ast.parse(line)\n",
    "        for node in ast.walk(ast_tree):\n",
    "            if isinstance(node, ast.Name):\n",
    "                identifiers.append(node.id)\n",
    "    except SyntaxError:\n",
    "        continue\n",
    "\n",
    "# count the frequency of each identifier\n",
    "identifier_counts = Counter(identifiers)\n",
    "\n",
    "# print the 50 most common identifiers\n",
    "for identifier, count in identifier_counts.most_common(50):\n",
    "    print(f\"{identifier}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar pairs of identifiers:\n",
      "os and a: 0.99\n",
      "print and os: 0.98\n",
      "print and a: 0.98\n",
      "t and a: 0.97\n",
      "t and os: 0.97\n",
      "data and os: 0.97\n",
      "data and a: 0.97\n",
      "print and t: 0.97\n",
      "print and data: 0.96\n",
      "t and data: 0.96\n",
      "\n",
      "Most dissimilar pairs of identifiers:\n",
      "x and xx: -0.23\n",
      "X_test and ax0: -0.24\n",
      "iris and b: -0.24\n",
      "ax1 and random_st: -0.24\n",
      "Y and y_train: -0.24\n",
      "plt and X_test: -0.24\n",
      "fig and b_: -0.25\n",
      "X_test and iris: -0.27\n",
      "A and i4: -0.31\n",
      "plt and y_train: -0.37\n"
     ]
    }
   ],
   "source": [
    "# get the 50 most common identifiers\n",
    "most_common_identifiers = [identifier for identifier, count in identifier_counts.most_common(50)]\n",
    "\n",
    "# calculate the similarity between each pair of the 50 most common identifiers\n",
    "similarities = {}\n",
    "for i, identifier1 in enumerate(most_common_identifiers):\n",
    "    for identifier2 in most_common_identifiers[i+1:]:\n",
    "        similarity = model.wv.similarity(identifier1, identifier2)\n",
    "        similarities[(identifier1, identifier2)] = similarity\n",
    "\n",
    "# sort the pairs of identifiers by their similarity score\n",
    "sorted_pairs = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print the 10 most similar pairs of identifiers\n",
    "print(\"Most similar pairs of identifiers:\")\n",
    "for pair, similarity in sorted_pairs[:10]:\n",
    "    print(f\"{pair[0]} and {pair[1]}: {similarity:.2f}\")\n",
    "\n",
    "# print the 10 most dissimilar pairs of identifiers\n",
    "print(\"\\nMost dissimilar pairs of identifiers:\")\n",
    "for pair, similarity in sorted_pairs[-10:]:\n",
    "    print(f\"{pair[0]} and {pair[1]}: {similarity:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e006b92cf6911bb559bf685cd6f76a0f36181bfcf0c5c8daa6ec7e16acca149"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
